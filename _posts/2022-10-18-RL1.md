---
layout: post
title: "강화학습 기초"
date: 2022-10-18 15:31:29 +0900
categories: Reinforcement
use_math: true
---

# 강화학습이란

강화학습을 정의하려면 행동심리학의 강화라는 개념과 머신러닝의 개념을 알아야 합니다.

행동심리학에서 강화라는 것은 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것입니다. 가까운 예로는 아이가 처음 걷는 것을 배울 때 걷는 방법을 배우는 것이 아니라 스스로 이것저것 시도해 보고 우연히 걷게 되면 처음에는 자신이 했던 특정한 행동과 걷게 된다는 보상을 연관짓지 못하여 다시 넘어지지만 시간이 지남에 따라 그 관계를 학습해서 결국 걸어 다니게 됩니다.

머신러닝은 기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습해서 실행할 수 있도록 하는 알고리즘을 개발하는 연구 분야입니다.

따라서, 컴퓨터가 환경과 직접적으로 상호작용하여 스스로 학습을 것을 강화학습이라고 합니다.

## 에이전트

강화학습을 통해 스스로 학습하는 컴퓨터를 에이전트라고 합니다.

<p align="center"><img src="/public/img/agent.png"></p>

![image](/public/img/agent.png){: width="20%" height="20%"}

<center><img src="/public/img/agent.png" width="50%" height="50%"></center>

위의 그림과 같이 에이전트는 자신의 행동과 그 행동에 대한 결과를 보상을 통해 학습하면서 어떤 행동이 좋은 결과를 야기하는지 알게 됩니다. 강화 학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 "최적의 행동양식, 또는 정책"을 학습하는 것입니다.

# MDP

어떠한 문제를 컴퓨터가 풀기 위해서는 문제를 수학적으로 정의해야 합니다. 강화학습이 풀고자 하는 순차적 행동 결정 문제는 **MDP**(Markov Decision Process)로 정의할 수 있습니다. MDP는 <u>상태(S), 행동(a), 보상(R), 정책($\pi$)</u>으로 구성되어 있습니다.

# 가치함수

강화학습에서 보상을 설명할때 아래 수식과 같이 설명합니다.
반환값 $G_t$는 어떤 시점에서 부터 받은 보상을 계산할떄 사용됩니다. 이를 이용해 어떤 시점에서의 가치(보상의 정도)를 계산 할 수 있는 것입니다.

$$G_t = R_{t+1}+\gamma R_{t+2} + \gamma ^2 R_{t+3} + ...$$

### - 상태 가치 함수

보상이 최대가 되는 정책을 찾으려면, 컴퓨터인 에이전트에게 어떤 상태 $S_t$에서 어떤 행동 $A_t$가 가치있는 행동인지를 알려줘야 합니다. 강화학습에서는 가치 함수 v(s)를 이용해 이 문제를 해결합니다.

$$
\begin{aligned}
v(s)&=E[G_t|S_t = s]\\
&=E[R_{t+1}+\gamma R_{t+2} + \gamma ^2 R_{t+3} + ... + |S_t=s]\\
&=E[R_{t+1}+\gamma G_{t+2} + |S_t=s]
\end{aligned}
$$

### - 행동 가치 함수(Q-function)

위에서 언급한 상태 가치 함수는 어떤 상태에서 받을 수 있는 보상을 나타내는 함수이고, Q-function이라 불리는 행동 가치 함수는 **어떤 상태에서 어떤 행동을 취했을 때, 얻게되는 보상에 대한 기대값** 이라고 할 수 있습니다. 그 기대값은 $q_\pi(s,a)$ 로 표현되고, 식은 다음과 같이 나타낼 수 있습니다.

$$
q_\pi(s,a) = E_\pi[R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) + |S_t =s, A_t = a]
$$

어떤 상태 s에서 모든 행동으로부터 오는 보상, 즉 모든 행동 가치 함수 $q_\pi(s,a)$ 와 그 행동을 할 확률 $\pi({a|s})$ 의 곱을 모두 더하면 상태가치함수가 되는것을 알 수 있습니다.

$$v_\pi(s) = \sum_{a \in A}{\pi(a|s)q_\pi(s,a)}$$

# 벨만 방정식

현재상태의 가치함수와 다음 상태의 가치함수의 관계식을 벨만 방정식이라고 합니다. 정책을 고려한 상태 가치 함수라고 생각하면 됩니다.

$$v_\pi(s)=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})+|S_t=s]$$

위의 벨만 기대 방정식을 통해 최적의 정책을 찾는 것은 어떤 상태에서 최적의 행동 a를 찾는 거라고 할 수 있습니다. 이는 행동가치함수, 상태가치함수 모두 최적의 선택을 해야합니다. 하지만 처음부터 한번에 찾는 것은 쉽지 않습니다. 아래 두 수식은 현재의 정책을 통해 최적의 q와 v를 선택하는 것이고, 에이전트는 학습시 자신의 경험을 기반으로 값을 업데이트 합니다.

$$
v^*(s)=max_\pi[v_pi(s)]\\
q^*(s,a)=max_\pi[q_pi(s,a)]
$$

그리고 현재의 정책 $\pi(s,a)$ 이 $q$값을 가장 크게 할 행동 a를 취한다는 것을 나타내는 식은 아래와 같다.

$\pi(s,a)=
\begin{cases}
1\quad if\;a=argmax_{a\in A}q^*(s,a)\\
0\quad otherwise
\end{cases}$

## 정책 이터레이션

## 가치 이터레이션

## 다이나믹 프로그래밍의 한계와 강화학습

# 강화학습과 정책 평가

## 몬테카를로 예측

## 시간차 예측

# 강화학습 알고리즘

## 살사

## 큐러닝
