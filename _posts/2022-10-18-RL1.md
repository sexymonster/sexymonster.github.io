---
layout: post
title: "강화학습 기초"
date: 2022-10-18 15:31:29 +0900
categories: Reinforcement
---

# 강화학습이란

강화학습을 정의하려면 행동심리학의 강화라는 개념과 머신러닝의 개념을 알아야 합니다.

행동심리학에서 강화라는 것은 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것입니다. 가까운 예로는 아이가 처음 걷는 것을 배울 때 걷는 방법을 배우는 것이 아니라 스스로 이것저것 시도해 보고 우연히 걷게 되면 처음에는 자신이 했던 특정한 행동과 걷게 된다는 보상을 연관짓지 못하여 다시 넘어지지만 시간이 지남에 따라 그 관계를 학습해서 결국 걸어 다니게 됩니다.

머신러닝은 기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습해서 실행할 수 있도록 하는 알고리즘을 개발하는 연구 분야입니다.

따라서, 컴퓨터가 환경과 직접적으로 상호작용하여 스스로 학습을 것을 강화학습이라고 합니다.

## 에이전트

강화학습을 통해 스스로 학습하는 컴퓨터를 에이전트라고 합니다.\
![alt text](/public/img/agent.png)\
위의 그림과 같이 에이전트는 자신의 행동과 그 행동에 대한 결과를 보상을 통해 학습하면서 어떤 행동이 좋은 결과를 야기하는지 알게 됩니다. 강화 학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 "최적의 행동양식, 또는 정책"을 학습하는 것입니다.

# MDP

어떠한 문제를 컴퓨터가 풀기 위해서는 문제를 수학적으로 정의해야 합니다. 강화학습이 풀고자 하는 순차적 행동 결정 문제는 **MDP**(Markov Decision Process)로 정의할 수 있습니다. MDP는 <u>상태(S), 행동(a), 보상(R), 정책($$\pi$$)</u>으로 구성되어 있습니다.

# 가치함수

### 상태 가치 함수

보상이 최대가 되는 정책을 찾으려면, 컴퓨터인 에이전트에게 어떤 상태 S~t~에서 어떤 행동 A~t~가 가치있는 행동인지를 알려줘야 합니다. 강화학습에서는 가치 함수 v(s)를 이용해 이 문제를 해결합니다.

$$
v(s)&=E[G~t~|S~t~ = s]
&=E[R~{t+1}~+\gamma R~{t+2}~ + \gamma ^2 R~{t+3}~ + ... + |S~t~=s]
&=E[R~{t+1}~+\gamma G~{t+2}~ + |S~t~=s]
$$

###

# 벨만 방정식

## 정책 이터레이션

## 가치 이터레이션

## 다이나믹 프로그래밍의 한계와 강화학습

# 강화학습과 정책 평가

## 몬테카를로 예측

## 시간차 예측

# 강화학습 알고리즘

## 살사

## 큐러닝
